{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e154e2-c962-4ce8-8962-2a8e129369f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.ci.artifacts.walmart.com/artifactory/api/pypi/pythonhosted-pypi-release-remote/simple\n",
      "Collecting regex\n",
      "  Downloading https://pypi.ci.artifacts.walmart.com/artifactory/api/pypi/pythonhosted-pypi-release-remote/packages/packages/14/62/b56d29e70b03666193369bdbdedfdc23946dbe9f81dd78ce262c74d988ab/regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
      "\u001b[2K     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 791.7/791.7 KB 6.8 MB/s eta 0:00:00\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading https://pypi.ci.artifacts.walmart.com/artifactory/api/pypi/pythonhosted-pypi-release-remote/packages/packages/b2/94/443fab3d4e5ebecac895712abd3849b8da93b7b7dec61c7db5c9c7ebe40c/tiktoken-0.12.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.2/1.2 MB 18.7 MB/s eta 0:00:001\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2025.11.3 tiktoken-0.12.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install regex tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe4cdfd-adca-4573-bb5d-67c97062174d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello world ðŸ’©\n",
      "Unicode Code Points: [72, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 32, 128169]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: Unicode Code Points\n",
    "text = \"Hello world ðŸ’©\"\n",
    "print(f\"Original Text: {text}\")\n",
    "\n",
    "# Extract Unicode Code Points\n",
    "code_points = [ord(char) for char in text]\n",
    "print(f\"Unicode Code Points: {code_points}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabee60e-5024-4a8e-aa51-01dd3b36fb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text String: Hello world ðŸ’©\n",
      "Byte List:   [72, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 32, 240, 159, 146, 169]\n",
      "Length of String: 13\n",
      "Length of Bytes:  16\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: UTF-8 Encoding\n",
    "text = \"Hello world ðŸ’©\"\n",
    "text_bytes = text.encode(\"utf-8\")\n",
    "byte_list = list(text_bytes)\n",
    "\n",
    "print(f\"Text String: {text}\")\n",
    "print(f\"Byte List:   {byte_list}\")\n",
    "print(f\"Length of String: {len(text)}\")\n",
    "print(f\"Length of Bytes:  {len(byte_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371be5b0-0c70-429e-8a79-9840ff672b92",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2745100473.py, line 74)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 74\u001b[0;36m\u001b[0m\n\u001b[0;31m    newids =\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Implementation: BasicTokenizer\n",
    "class BasicTokenizer:\n",
    "    def __init__(self):\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.vocab = {}  # int -> bytes\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "        \n",
    "        # Preprocessing: Convert string to UTF-8 bytes\n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        ids = list(text_bytes) # Integers 0-255\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            stats = get_stats(ids)\n",
    "            if not stats:\n",
    "                break\n",
    "            \n",
    "            # Select the most frequent pair\n",
    "            pair = max(stats, key=stats.get)\n",
    "            \n",
    "            # Mint a new token ID\n",
    "            idx = 256 + i\n",
    "            \n",
    "            # Apply the merge\n",
    "            ids = merge(ids, pair, idx)\n",
    "            \n",
    "            # Record the merge\n",
    "            self.merges[pair] = idx\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Merge {i+1}/{num_merges}: {pair} -> {idx} (Count: {stats[pair]})\")\n",
    "        \n",
    "        # Build the final vocabulary map for decoding\n",
    "        # 1. Initialize with raw bytes\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        # 2. Add merged tokens recursively\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            self.vocab[idx] = self.vocab[p0] + self.vocab[p1]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # Concatenate bytes\n",
    "        tokens = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        # Decode to string, handling invalid byte sequences gracefully\n",
    "        text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "\n",
    "    def encode(self, text):\n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        ids = list(text_bytes)\n",
    "        \n",
    "        while len(ids) >= 2:\n",
    "            stats = get_stats(ids)\n",
    "            # Find the pair in the current sequence with the lowest merge index\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            \n",
    "            # If the best pair isn't in our learned merges, we are done\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            \n",
    "            idx = self.merges[pair]\n",
    "            ids = merge(ids, pair, idx)\n",
    "        return ids\n",
    "\n",
    "# Helper functions required by the class\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): \n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b190ce33-03d0-46a0-8349-c5410ec62363",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BasicTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Exercise 3: Training the BasicTokenizer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBasicTokenizer\u001b[49m()\n\u001b[1;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maaabdaaabac\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BasicTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Exercise 3: Training the BasicTokenizer\n",
    "tokenizer = BasicTokenizer()\n",
    "text = \"aaabdaaabac\"\n",
    "print(f\"Input: {text}\")\n",
    "\n",
    "# Train for 3 merges (256 + 3 = 259 vocab size)\n",
    "# We expect the most frequent pairs to be merged first.\n",
    "tokenizer.train(text, vocab_size=259, verbose=True)\n",
    "\n",
    "# Encode\n",
    "encoded = tokenizer.encode(text)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "\n",
    "# Decode\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "# Verify\n",
    "assert text == decoded\n",
    "print(\"Success: Decoded string matches input.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a565c93d-1f26-46e3-a06f-7988cc727689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re # 'regex' module is required for \\p{...} support\n",
    "\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)|?\\p{L}+|?\\p{N}+|?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76e365b2-e3ad-4b19-8dd0-63d7bd508924",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}|?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2488b0-136a-47a9-bca9-8532befe5f96",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1950933026.py, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 43\u001b[0;36m\u001b[0m\n\u001b[0;31m    ids =\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class RegexTokenizer(BasicTokenizer):\n",
    "    def __init__(self, pattern=None):\n",
    "        super().__init__()\n",
    "        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
    "        self.compiled_pattern = re.compile(self.pattern)\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "        \n",
    "        # 1. Split text into chunks using regex\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "        \n",
    "        # 2. Convert chunks to lists of byte integers\n",
    "        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            stats = {}\n",
    "            # Count pairs across ALL chunks\n",
    "            for chunk_ids in ids:\n",
    "                # Update stats dictionary in place\n",
    "                for pair in zip(chunk_ids, chunk_ids[1:]):\n",
    "                    stats[pair] = stats.get(pair, 0) + 1\n",
    "            \n",
    "            if not stats:\n",
    "                break\n",
    "            \n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            \n",
    "            # Apply merge to ALL chunks\n",
    "            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
    "            \n",
    "            self.merges[pair] = idx\n",
    "            \n",
    "        # Build vocabulary\n",
    "        self.vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            self.vocab[idx] = self.vocab[p0] + self.vocab[p1]\n",
    "\n",
    "    def encode(self, text):\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "        ids = []\n",
    "        for chunk in text_chunks:\n",
    "            chunk_bytes = chunk.encode(\"utf-8\")\n",
    "            chunk_ids = list(chunk_bytes)\n",
    "            # Apply BPE merges to this specific chunk\n",
    "            chunk_ids = self._encode_chunk(chunk_ids)\n",
    "            ids.extend(chunk_ids)\n",
    "        return ids\n",
    "\n",
    "    def _encode_chunk(self, ids):\n",
    "        # Same encoding logic as BasicTokenizer, but for a single chunk\n",
    "        while len(ids) >= 2:\n",
    "            stats = get_stats(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            ids = merge(ids, pair, idx)\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7a6381f-8d69-4c99-91c6-3560a1bda928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Text: Hello, world! 12345\n",
      "--- Basic Tokenizer ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BasicTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Train Basic Tokenizer\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Basic Tokenizer ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m basic_enc \u001b[38;5;241m=\u001b[39m \u001b[43mBasicTokenizer\u001b[49m()\n\u001b[1;32m      8\u001b[0m basic_enc\u001b[38;5;241m.\u001b[39mtrain(text, \u001b[38;5;241m260\u001b[39m) \u001b[38;5;66;03m# Train slightly\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoded:\u001b[39m\u001b[38;5;124m\"\u001b[39m, basic_enc\u001b[38;5;241m.\u001b[39mencode(text))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BasicTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Exercise 4: Basic vs Regex Tokenizer\n",
    "text = \"Hello, world! 12345\"\n",
    "print(f\"Test Text: {text}\")\n",
    "\n",
    "# Train Basic Tokenizer\n",
    "print(\"--- Basic Tokenizer ---\")\n",
    "basic_enc = BasicTokenizer()\n",
    "basic_enc.train(text, 260) # Train slightly\n",
    "print(\"Encoded:\", basic_enc.encode(text))\n",
    "\n",
    "# Train Regex Tokenizer\n",
    "print(\"--- Regex Tokenizer ---\")\n",
    "regex_enc = RegexTokenizer()\n",
    "regex_enc.train(text, 260) # Train slightly\n",
    "print(\"Encoded:\", regex_enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cfb6e36-3fcd-4f02-b709-bff8b0e36f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_reconstruct(mergeable_ranks, token, max_rank):\n",
    "    parts = [bytes([b]) for b in token]\n",
    "    while True:\n",
    "        min_idx = None\n",
    "        min_rank = None\n",
    "        # Find the pair with the lowest rank (highest priority)\n",
    "        for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
    "            rank = mergeable_ranks.get(pair + pair[1])\n",
    "            if rank is not None and (min_rank is None or rank < min_rank):\n",
    "                min_idx = i\n",
    "                min_rank = rank\n",
    "        if min_rank is None or (max_rank is not None and min_rank >= max_rank):\n",
    "            break\n",
    "        assert min_idx is not None\n",
    "        # Merge\n",
    "        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2:]\n",
    "    return parts\n",
    "\n",
    "def recover_merges(mergeable_ranks):\n",
    "    merges = {}\n",
    "    for token, rank in mergeable_ranks.items():\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        # Reconstruct how this token was built\n",
    "        pair = tuple(bpe_reconstruct(mergeable_ranks, token, max_rank=rank))\n",
    "        assert len(pair) == 2\n",
    "        \n",
    "        ix0 = mergeable_ranks[pair]\n",
    "        ix1 = mergeable_ranks[pair[1]]\n",
    "        merges[(ix0, ix1)] = rank\n",
    "    return merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9522f909-b9db-448b-910a-235780f20ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Error: Encountered text corresponding to disallowed special token '<|endoftext|>'.\n",
      "If you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\n",
      "If you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\n",
      "To disable this check for all special tokens, pass `disallowed_special=()`.\n",
      "\n",
      "Tokens with special: [9906, 220, 100257, 1917]\n",
      "ID for <|endoftext|>: 220\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5: Tiktoken and Special Tokens\n",
    "import tiktoken\n",
    "\n",
    "# Load GPT-4 tokenizer\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Test encoding with special tokens\n",
    "text = \"Hello <|endoftext|> world\"\n",
    "try:\n",
    "    # This will fail by default for safety\n",
    "    print(enc.encode(text)) \n",
    "except Exception as e:\n",
    "    print(f\"Expected Error: {e}\")\n",
    "\n",
    "# Allow special tokens\n",
    "tokens = enc.encode(text, allowed_special=\"all\")\n",
    "print(f\"Tokens with special: {tokens}\")\n",
    "\n",
    "# Identify the ID for <|endoftext|>\n",
    "eot_id = tokens[1]\n",
    "print(f\"ID for <|endoftext|>: {eot_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5f53e-ddd9-4db1-94bc-7a4574bf0926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
