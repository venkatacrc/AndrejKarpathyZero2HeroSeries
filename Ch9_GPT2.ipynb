{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee577c-0274-4ca4-9352-cf54186fd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fineweb.py: Downloads and tokenizes FineWeb-EDU data.\n",
    "\"\"\"\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "local_dir = \"edu_fineweb10B\"\n",
    "remote_name = \"sample-10BT\"\n",
    "shard_size = int(1e8) # 100M tokens per shard\n",
    "\n",
    "# Create the cache directory if it doesn't exist yet\n",
    "DATA_CACHE_DIR = os.path.join(os.path.dirname(__file__), local_dir)\n",
    "os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Download the dataset\n",
    "fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=remote_name, split=\"train\")\n",
    "\n",
    "# Initialize the tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eot = enc._special_tokens['<|endoftext|>'] # 50256\n",
    "\n",
    "def tokenize(doc):\n",
    "    # Tokenize a single document and return a numpy array of uint16 tokens\n",
    "    tokens = [eot] # the special <|endoftext|> token delimits all documents\n",
    "    tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
    "    tokens_np = np.array(tokens)\n",
    "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "    return tokens_np.astype(np.uint16)\n",
    "\n",
    "def write_datafile(filename, tokens_np):\n",
    "    np.save(filename, tokens_np)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nprocs = max(1, os.cpu_count() // 2)\n",
    "    with mp.Pool(nprocs) as pool:\n",
    "        shard_index = 0\n",
    "        all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
    "        token_count = 0\n",
    "        progress_bar = None\n",
    "        \n",
    "        for tokens in pool.imap(tokenize, fw, chunksize=16):\n",
    "            if token_count + len(tokens) < shard_size:\n",
    "                all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
    "                token_count += len(tokens)\n",
    "                if progress_bar is None:\n",
    "                    progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
    "                progress_bar.update(len(tokens))\n",
    "            else:\n",
    "                split = \"val\" if shard_index == 0 else \"train\"\n",
    "                filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
    "                remainder = shard_size - token_count\n",
    "                progress_bar.update(remainder)\n",
    "                all_tokens_np[token_count:token_count+remainder] = tokens[:remainder]\n",
    "                write_datafile(filename, all_tokens_np)\n",
    "                shard_index += 1\n",
    "                progress_bar = None\n",
    "                all_tokens_np[0:len(tokens)-remainder] = tokens[remainder:]\n",
    "                token_count = len(tokens)-remainder\n",
    "        \n",
    "        if token_count!= 0:\n",
    "            split = \"val\" if shard_index == 0 else \"train\"\n",
    "            filename = os.path.join(DATA_CACHE_DIR, f\"edufineweb_{split}_{shard_index:06d}\")\n",
    "            write_datafile(filename, all_tokens_np[:token_count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeada9cc-3a4d-4406-a943-c1208f870dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # Rounded up from 50257 to be multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        # Flash Attention\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList(),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # Weight tying\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        x = self.transformer.wte(idx) + self.transformer.wpe(pos)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "        \n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        # Filter params that require grad\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "        # Decay: 2D tensors (weights), No Decay: 1D tensors (biases, layernorms)\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        # Use fused AdamW if available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and 'cuda' in device_type\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f28571-ff9a-4003-949a-6d11a3c2537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import ctypes\n",
    "import sys\n",
    "\n",
    "# Load CUDA libraries explicitly before torch imports them\n",
    "cuda_lib_paths = [\n",
    "    '/usr/local/nvidia/lib64/libcuda.so',\n",
    "    '/usr/local/lib/python3.10/dist-packages/nvidia/cuda_runtime/lib/libcudart.so.12'\n",
    "]\n",
    "for lib_path in cuda_lib_paths:\n",
    "    if os.path.exists(lib_path):\n",
    "        try:\n",
    "            ctypes.CDLL(lib_path)\n",
    "        except OSError:\n",
    "            pass  # Library might already be loaded\n",
    "\n",
    "# Also set LD_LIBRARY_PATH for any remaining libraries\n",
    "if 'LD_LIBRARY_PATH' not in os.environ or '/usr/local/nvidia/lib64' not in os.environ.get('LD_LIBRARY_PATH', ''):\n",
    "    cuda_lib_dirs = [\n",
    "        '/usr/local/nvidia/lib64',\n",
    "        '/usr/local/lib/python3.10/dist-packages/nvidia/cuda_runtime/lib'\n",
    "    ]\n",
    "    existing_path = os.environ.get('LD_LIBRARY_PATH', '')\n",
    "    os.environ['LD_LIBRARY_PATH'] = ':'.join(cuda_lib_dirs + ([existing_path] if existing_path else []))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "# Import the model definition from step 3 (assuming it's in the same file or imported)\n",
    "# For this single-file reproduction, paste the classes from Section 3.2 here.\n",
    "from model import GPT, GPTConfig\n",
    "\n",
    "# --- DataLoader ---\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        assert split in {'train', 'val'}\n",
    "        data_root = \"edu_fineweb10B\"\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        self.shards = [os.path.join(data_root, s) for s in shards]\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_shard = 0\n",
    "        self.tokens = self.load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = 0\n",
    "\n",
    "    def load_tokens(self, filename):\n",
    "        npt = np.load(filename).astype(np.int32)\n",
    "        return torch.tensor(npt, dtype=torch.long)\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        if self.current_position + B * T + 1 > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = self.load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = 0\n",
    "        \n",
    "        # Slice the tokens starting from current_position\n",
    "        buf = self.tokens[self.current_position:self.current_position + B * T + 1]\n",
    "        x = buf[:-1].view(B, T)\n",
    "        y = buf[1:].view(B, T)\n",
    "        self.current_position += B * T\n",
    "        return x, y\n",
    "\n",
    "# --- Configuration ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_float32_matmul_precision('high') # TF32\n",
    "\n",
    "total_batch_size = 524288 # ~0.5M tokens\n",
    "B = 16 # Micro batch size (adjust if OOM)\n",
    "T = 1024\n",
    "grad_accum_steps = total_batch_size // (B * T)\n",
    "max_steps = 19073 # 10B tokens / 0.5M batch size\n",
    "\n",
    "train_loader = DataLoaderLite(B=B, T=T, split=\"train\")\n",
    "val_loader = DataLoaderLite(B=B, T=T, split=\"val\")\n",
    "\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "model.to(device)\n",
    "model = torch.compile(model) # Compiler optimization\n",
    "\n",
    "# LR Schedule\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 715\n",
    "def get_lr(it):\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it+1) / warmup_steps\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device)\n",
    "\n",
    "# --- Training Loop ---\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Validation every 100 steps\n",
    "    if step % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_steps = 20\n",
    "            for _ in range(val_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(x, y)\n",
    "                val_loss_accum += loss.item()\n",
    "            print(f\"validation loss: {val_loss_accum/val_steps:.4f}\")\n",
    "\n",
    "    # Training Step\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "            loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T * grad_accum_steps) / (t1 - t0)\n",
    "    print(f\"step {step:5d} | loss {loss_accum:.4f} | lr {lr:.4e} | norm {norm:.4f} | dt {dt:.2f}ms | tok/sec {tokens_per_sec:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a366946a-410f-4e4d-b9d6-5dd6a39fd821",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_gpt2.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
